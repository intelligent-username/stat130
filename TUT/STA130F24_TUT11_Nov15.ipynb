{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1c5662f",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## STA130 TUT 08 (Nov15)<br><br> ðŸŒ³ðŸŒ² <u>Decision Tree Classification with _sklearn_ and confusion matrices<u>\n"
=======
    "# STA130 TUT 11 (Nov15)<br><br> ðŸŒ³ðŸŒ² <u>Decision Tree Classification <br>with _sklearn_ and confusion matrices<u>\n"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8cc949",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### â™»ï¸ ðŸ“š Review  / Questions [30 minutes]\n",
    "\n",
    "1. **[15 of the 30 minutes]** Follow up clarification questions regarding **regression**?  Which **classification** introduced today builds directly upon?\n",
    "\n",
    "> 1. How are **predictors variables** used to **predict** an **outcome variable**?\n",
    "> 2. What's the difference between **predictive performance** and **coefficient hypothesis testing** using p-values?\n",
    "> 3. Can **R-squared** ever get worse with each additional predictor variable added to the model?\n",
    "> 4. What kind of **model performance evaluation** are we really interested in? \n",
    "> 5. Etc.?\n",
    "\n",
    "2. **[15 of the 30 minutes]** **Train-Test** \"in sample\" versus \"out of sample\" validation\n"
=======
    "## â™»ï¸ ðŸ“š Review  / Questions [30 minutes]\n",
    "\n",
    "### 1. **[15 of the 30 minutes]** Follow up clarification questions regarding **multiple linear regression**?  Which **classification** introduced today builds directly upon...\n",
    "\n",
    "> 1. How are **predictors variables** used to **predict** an **outcome variable**? E.g.,\n",
    "> \n",
    ">    $$Y_i = \\beta_0 + \\beta_1x_i + \\beta_2 1_{B}(k_i) + \\beta_3 1_{C}(k_i) + \\beta_4x_i1_{B}(k_i) + \\beta_5 1_{B}(k_i)1_{C}(k_i) + \\beta_6x_i^2$$\n",
    "> \n",
    ">\n",
    "> 2. What's the difference between **predictive performance** and **coefficient hypothesis testing** using p-values?\n",
    "> 3. Can **R-squared** ever get worse with each additional predictor variable added to the model?\n",
    "\n",
    "### 2. **[15 of the 30 minutes]** **Train-Test** \"in sample\" versus \"out of sample\" validation\n"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60dfcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c468b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data = datasets.load_breast_cancer()\n",
    "cancer_df = pd.DataFrame(data=cancer_data.data, \n",
    "                         columns=cancer_data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0670a35c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Notice the binary 'target' \n",
    "# with 'target_names': array(['malignant', 'benign'], dtype='<U9')\n",
    "cancer_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c29e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer_data['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "id": "6ae9c6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in cancer_df:\n",
    "    if all(cancer_df[i]>0):\n",
    "        cancer_df[i]=np.log(cancer_df[i])\n",
    "\n",
    "_ = cancer_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> upstream/main
   "id": "39f6a540",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# Randomly split into 80% training data and 20% testing data\n",
    "# Why?\n",
    "np.random.seed(131)\n",
    "training_indices = cancer_df.sample(frac=0.80, replace=False).index.sort_values()\n",
=======
    "# Randomly split into 50% training data and 50% testing data\n",
    "# Why?\n",
    "np.random.seed(130)\n",
    "training_indices = cancer_df.sample(frac=0.5, replace=False).index.sort_values()\n",
>>>>>>> upstream/main
    "testing_indices = cancer_df.index[~cancer_df.index.isin(training_indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "ab647fb4",
=======
   "id": "bd546ea6",
>>>>>>> upstream/main
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall the binary 'target' \n",
    "# But notice here the outcome variable is continuous (not binary) \n",
    "# (and same for the predictor variables, binary 'target' not used)\n",
<<<<<<< HEAD
    "linear_spec = 'Q(\"mean compactness\") ~ Q(\"mean radius\") + Q(\"mean concavity\")'               \n",
    "MLR = smf.ols(linear_spec, data=cancer_df.loc[training_indices,:])\n",
    "MLR_fit = MLR.fit() # Fit the mulitple lienar regression model (MLR)"
=======
    "linear_spec_4 = '''\n",
    "scale(Q('mean area')) ~ scale(Q('texture error')) + scale(Q('smoothness error'))\n",
    "                      + scale(Q('mean fractal dimension')) + scale(Q('mean smoothness'))\n",
    "                      + scale(Q('mean symmetry')) * scale(Q('area error'))\n",
    "                      * scale(Q('worst texture'))\n",
    "                      * scale(Q('worst smoothness'))\n",
    "                      * scale(Q('worst symmetry'))\n",
    "                      * scale(Q('worst concave points'))\n",
    "                      * scale(Q('worst compactness'))\n",
    "                      * scale(Q('worst concavity'))\n",
    "'''\n",
    "\n",
    "linear_spec_3 = '''\n",
    "scale(Q('mean area')) ~ scale(Q('texture error')) * scale(Q('smoothness error'))\n",
    "                      * scale(Q('mean fractal dimension')) * scale(Q('mean smoothness'))\n",
    "                      * scale(Q('mean symmetry')) \n",
    "'''\n",
    "\n",
    "linear_spec_2 = '''\n",
    "scale(Q('mean area')) ~ scale(Q('texture error')) + scale(Q('smoothness error'))\n",
    "                      + scale(Q('mean fractal dimension')) + scale(Q('mean smoothness'))\n",
    "                      + scale(Q('mean symmetry')) \n",
    "'''\n",
    "linear_spec_1 = '''\n",
    "scale(Q('mean area')) ~ scale(Q('texture error')) + scale(Q('smoothness error'))\n",
    "'''\n",
    "MLR = smf.ols(linear_spec_3, data=cancer_df.loc[training_indices,:])\n",
    "MLR_fit = MLR.fit() # Fit the mulitple lienar regression model (MLR)\n"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "a45e4800",
=======
   "id": "a7c80436",
>>>>>>> upstream/main
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"In sample\" performance based on the \"training data\"\n",
    "np.corrcoef(MLR_fit.predict(cancer_df.loc[training_indices,:]),\n",
<<<<<<< HEAD
    "            cancer_df.loc[training_indices,\"mean compactness\"])[0,1]**2"
=======
    "            cancer_df.loc[training_indices,\"mean area\"])[0,1]**2"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b1421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Out of sample\" performance based on the \"testing data\"\n",
    "# Why?\n",
    "np.corrcoef(MLR_fit.predict(cancer_df.loc[testing_indices,:]),\n",
<<<<<<< HEAD
    "            cancer_df.loc[testing_indices,\"mean compactness\"])[0,1]**2"
=======
    "            cancer_df.loc[testing_indices,\"mean area\"])[0,1]**2"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "b69173a7",
   "metadata": {},
   "source": [
    "- The fact that the model fit as evaluated by R-squared (the \"proportion of variation explained\") is better \"data the models was trained on and has already seen\" and worse for \"new\" data is generally what we'd expect to see in an analysis like this... \n",
    "    - Why?\n",
    "\n",
    "\n",
    "- The good news is in the example above is that the \"out of sample\" predictive \"proportion of variation explained\" performance is not much worse than the \"in sample\" performance...\n",
    "- Which we'd typically interpret as suggesting that the model seems to represent the data fairly well...\n",
    "    - Why?\n",
    "\n",
    "\n",
    "- Are there any concerns about the validity of the ostinsible conclusions above? \n",
    "\n",
    "    - What might those be if so?\n",
=======
   "id": "e629ee27",
   "metadata": {},
   "source": [
    "- `linear_spec_1`: The model fit as evaluated by R-squared the \"proportion of variation explained\" is better for data the model was trained on and worse for new data \n",
    "    - Why might this be expected?  What explains this?\n",
    "\n",
    "\n",
    "- `linear_spec_2`: The \"in sample\" performance remains better compared to the \"out of sample\" performance, but overall performance is improved and relatively speaking the \"out of sample\" performance \n",
    "    - Why might both of these be expected?  What explains these?\n",
    "\n",
    "\n",
    "- `linear_spec_3`: something has gone wrong\n",
    "    - `linear_spec_4`: something has gone very wrong\n",
    "\n",
    "\n",
    "- When \"out of sample\" predictive \"proportion of variation explained\" performance is not much worse than the \"in sample\" performance, then this suggests that the model seems to represent the data fairly well...\n",
    "    - Why?\n",
    "\n",
>>>>>>> upstream/main
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "ac3008db",
   "metadata": {},
   "source": [
    "### ðŸš§ ðŸ—ï¸ Demo (Introducing Classification Decision Trees and Confusion Matrices) [30 minutes] "
=======
   "id": "4f1b059f",
   "metadata": {},
   "source": [
    "## ðŸš§ ðŸ—ï¸ Demo (Introducing Classification Decision Trees and Confusion Matrices) [30 minutes] "
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "0d2df19c",
   "metadata": {},
   "source": [
    "#### 1. **[15 of the 30 minutes]** The concept of a *classification decision tree*\n",
=======
   "id": "4420888a",
   "metadata": {},
   "source": [
    "### 1. **[12 of the 30 minutes]** The concept of a *classification decision tree*\n",
>>>>>>> upstream/main
    "\n",
    "Some questions to keep in mind (and hopefully answer) as you go\n",
    "\n",
    "- What's similar about this code and the **multiple linear regression code** above? \n",
    "- A **classification decision tree** is a model like **multiple linear regression**... how so?\n",
    "- **Predictions** of **classification decision tree** are made differently than those of **multiple linear regression**... how so?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "51e0b8e0",
=======
   "id": "6f62f392",
>>>>>>> upstream/main
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "# Initialize and train the Decision Tree classifier\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "\n",
    "# Specify outcome and \"Design Matrix\"\n",
    "# y ~ col1 + col2 + ... + colk\n",
    "clf.fit(X=cancer_df.iloc[training_indices, :], \n",
<<<<<<< HEAD
    "        y=cancer_data.target[training_indices])"
=======
    "        y=cancer_data.target[training_indices])\n",
    "# The outcome has changed compared to the earlier multiple linear regression model..."
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "29619d3f",
=======
   "id": "5eb0d2d0",
>>>>>>> upstream/main
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details regarding code are not the point and will be discused and explore later in LEC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 4))\n",
    "plot_tree(clf, feature_names=cancer_data.feature_names.tolist(), \n",
    "          class_names=cancer_data.target_names.tolist(), \n",
    "          filled=True, rounded=True)\n",
    "plt.title(\"Decision Tree (Depth = 2)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452300ea",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "####  2. **[15 of the 30 minutes]** The **concept** of confusion matrix... \n",
    "\n",
    "> - Some questions to keep in mind (and hopefully answer) as you go\n",
    "> - Details regarding code are not the point and will be discused and explore later in LEC\n",
    "\n",
    "- **Classification** are **regression** are distinguised by the nature of the **outcome variables** they predict... how so? \n",
=======
    "###  2. **[18 of the 30 minutes]** The **concept** of confusion matrix... \n",
    "\n",
    "> Details regarding code are not the point and will be discused and explore later in LEC\n",
    "\n",
    "Some questions to keep in mind (and hopefully answer) as you go\n",
    "\n",
    "- **Classification** and **regression** are distinguised by the nature of the **outcome variables** they predict... How so? \n",
>>>>>>> upstream/main
    "- What should be similar about **model performance evaluation** for **classification decision tree** to what we've seen previously?\n",
    "- Are multiple kinds of **model performance evaluation metrics** possible for **classification decision tree** based on the **confusion matrix**?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "26e0f5c7",
=======
   "id": "73b101bd",
>>>>>>> upstream/main
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Benign 0 and Malignant 1\n",
    "# JUST RUN THIS ONCE!!\n",
    "cancer_data.target = 1-cancer_data.target \n",
    "cancer_data.target_names = np.array(['Benign\\n(negative)','Malignant\\n(positive)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "f6893ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details regarding code are not the point and will be discused and explore later in LEC\n",
=======
   "id": "f459aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details regarding code are not the point and will be discused and explored later in LEC\n",
>>>>>>> upstream/main
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_depth_2 = clf.predict(cancer_df.iloc[testing_indices, :])\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(cancer_data.target[testing_indices], y_pred_depth_2)\n",
    "\n",
    "# Get the target names for 'benign' and 'malignant'\n",
    "target_names = cancer_data.target_names.tolist()\n",
    "\n",
    "# Set up a confusion matrix with proper labels using target names\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[f'Predicted {label}' for label in target_names], \n",
    "            yticklabels=[f'Actually {label}' for label in target_names])\n",
    "\n",
    "plt.title('Confusion Matrix (Depth = 2)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "# Add custom labels for FP, FN, TP, and TN\n",
    "plt.text(0.5, 0.1, \"True Negative (TN)\", fontsize=12, color='red', ha='center', va='center')\n",
    "plt.text(1.5, 0.1, \"False Positive (FP)\", fontsize=12, color='red', ha='center', va='center')\n",
    "plt.text(0.5, 1.1, \"False Negative (FN)\", fontsize=12, color='red', ha='center', va='center')\n",
    "plt.text(1.5, 1.1, \"True Positive (TP)\", fontsize=12, color='red', ha='center', va='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ce69a7",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### ðŸ’¬ ðŸ—£ï¸ Communication Activity **[40 minutes]**\n",
    "\n",
    "1. **[7 minutes]** Break into your **course project groups** and confer regarding the purpose of the **train-test** \"in sample versus out-of-sample\" **validation** framework.\n",
    "\n",
    "2. **[6 minutes]** Identify an answer to the previous question which all groups in agreement are as close to unanimously satisfied with as possible (in the allotted time). \n",
    "   \n",
    "3. **[7 minutes]** Provide an example scenario in which the implications of **Type I erros** (wrongly rejecting the **null hypothesis**) and **Type II errors** (failing to reject a **false null hypothesis**) can be demonstrated. \n",
    "\n",
    "4. **[8 minutes]** Provide an example context involving **predictions** using a **classification decision tree** in which the implications of **false positive (FP)** and **false negative (FP) predictions** can be discussed and considered.\n",
    "\n",
    "5. **[6 minutes]** Explore with all groups what analogy they have (or could have) noticed between **Type I/Type II erros** and **false positive/negative (FP/FN) predictions**. \n",
    "\n",
    "6. **[6 minutes]** Explore with all groups what they believe the  meaningfulness, purpose, or relevance of the **train-test** \"in sample versus out-of-sample\" **validation** framework is relative to the **errors** considered here.\n",
    "\n",
    "7. If time permits, explore with all the groups whether or not they have identified any notion of different costs or consequences of the different kinds of errors that might be made.\n"
=======
    "## ðŸ’¬ ðŸ—£ï¸ Communication Activity **[40 minutes]**\n",
    "\n",
    "1. **[7 minutes]** Break into your **course project groups** and confer regarding the purpose of the **train-test** \"in sample versus out-of-sample\" **validation** framework.\n",
    "\n",
    "\n",
    "2. **[6 minutes]** Identify an answer to the previous question which all groups in agreement are as close to unanimously satisfied with as possible (in the allotted time). \n",
    "\n",
    "\n",
    "3. **[7 minutes]** Have two group provide an example scenario in which the implications of **Type I errors** (wrongly rejecting the **null hypothesis**) and **Type II errors** (failing to reject a **false null hypothesis**) can be demonstrated. \n",
    "\n",
    "\n",
    "4. **[8 minutes]** Have to other groups provide an example context involving **predictions** using a **classification decision tree** in which the implications of **false positive (FP)** and **false negative (FP) predictions** can be discussed and considered.\n",
    "\n",
    "\n",
    "5. **[6 minutes]** Lead a discussion with all groups exploring the analogy (and distinction) between **Type I/Type II errors** and **false positive/negative (FP/FN) predictions**. \n",
    "\n",
    "\n",
    "6. **[6 minutes]** Lead a discussion with all groups exploring what they believe the  meaningfulness, purpose, or relevance of the **train-test** \"in sample versus out-of-sample\" **validation** framework is relative to the **errors** considered here.\n",
    "\n",
    "\n",
    "7. If time permits, explore with all the groups whether or not they have identified any notion of a difference between the natures of the consequences of **false positive** versus **false negative (FP/FN) predictions**.\n"
>>>>>>> upstream/main
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
